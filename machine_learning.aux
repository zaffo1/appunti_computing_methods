\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning}{203}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\textit  {Gio 3 novembre - Lezione 12}}{203}{section*.392}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Introduction to Machine Learning}{203}{section*.393}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Topics}{203}{section*.394}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Machine Learning Basics}{203}{section*.395}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Types of typical ML problems}{204}{section*.396}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Function approximation}{204}{section*.397}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Model and Hyper-parameters}{204}{section*.399}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Parameters}{205}{section*.400}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Objective function}{205}{section*.401}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Objective function: binary cross entropy}{205}{section*.402}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Learning / Training}{206}{section*.405}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised learning}{206}{section*.406}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised learning}{206}{section*.409}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised vs unsupervised}{207}{section*.411}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reinforcement learning (not covered in this lectures)}{207}{section*.414}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Capacity and representational power}{207}{section*.416}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Generalization}{208}{section*.420}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Regularization}{209}{section*.423}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Hyperparameters(model) optimization}{209}{section*.425}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{K-folding cross validation}{209}{section*.427}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Nella figura i dati sono divisi solo in 2 (test e training), ma si possono dividere in 3 come abbiamo visto prima\relax }}{210}{figure.caption.428}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Inference}{210}{section*.429}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Accuracy, Precision, Sensitivity, Specificity}{210}{section*.430}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Examples of ML techniques}{210}{section*.432}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Linear regression (Supervised)}{210}{section*.433}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Principal Component Analysis (aka PCA) (Unsupervised)}{211}{section*.434}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nearest neighbors}{211}{section*.437}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  Figures from \url  {https://scikit-learn.org/stable/modules/neighbors.html}. \relax }}{211}{figure.caption.438}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Decision trees}{212}{section*.439}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Ensembles of trees}{212}{section*.441}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Bagging\relax }}{212}{figure.caption.443}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Gradient Boosting\relax }}{213}{figure.caption.444}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Limitations of decision trees}{213}{section*.445}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Many more ML techniques!}{213}{section*.447}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{What do we need to create our first ML program}{213}{section*.449}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Hands-on}{214}{section*.450}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Lun 7 novembre - lezione 13}}{241}{section*.451}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Introduction to Artificial Neural Networks}{241}{section*.452}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Artificial Neural Networks}{241}{section*.453}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{(Artificial) neural networks: the “Model”}{241}{section*.454}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Brief history, highs and lows}{241}{section*.456}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Complexity growth}{242}{section*.458}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Performance on classic problems}{243}{section*.460}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{My favorite performance examples}{243}{section*.462}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{OpenAI GPT3}{243}{section*.463}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Neural Nets Basic elements}{243}{section*.464}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{A neural network node: the artificial neuron}{243}{section*.465}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{The MLP model}{244}{section*.467}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Universal approximation theorem}{244}{section*.469}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Example (1-D input)}{245}{section*.470}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces \url  {https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6}\relax }}{245}{figure.caption.472}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training of an MLP}{245}{section*.473}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training a NN}{246}{section*.475}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{How to find a minimum?}{246}{section*.477}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Not as simple as you would imagine}{246}{section*.479}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Learning rate, epochs and batches}{247}{section*.481}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training and overfitting}{248}{section*.484}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Neural Networks, computers and mathematics}{248}{section*.486}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Back-propagation}{248}{section*.487}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Deep Networks}{249}{section*.489}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Deep Feed Forward networks}{249}{section*.490}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Why going deeper?}{249}{section*.492}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Activation functions}{250}{section*.493}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Deep architectures}{251}{section*.495}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Dropout and regularization methods}{252}{section*.497}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{(Batch) normalization}{252}{section*.499}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{DNN Tools}{253}{section*.501}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras}{253}{section*.502}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Other common tools}{253}{section*.503}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras Sequential example}{253}{section*.504}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras “Model” Functional API}{253}{section*.506}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{A (modernized) MLP in keras}{254}{section*.508}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{From the 1995 to 2010}{254}{section*.510}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Training a model with Keras}{255}{section*.511}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Keras Layers}{255}{section*.512}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras basic layers}{255}{section*.513}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Callbacks}{256}{section*.514}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 1}}{256}{section*.515}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 2}}{256}{section*.517}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Giovedì 10 novembre - Lezione 14}}{257}{section*.519}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Convolutional and recurrent networks}{257}{section*.520}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Classification of images}{257}{section*.521}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Exploit invariance and locality}{257}{section*.524}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Can we exploit problem invariance?}{258}{section*.526}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Limitations}{258}{section*.528}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Understanding the dimensions of the convolution}{259}{section*.529}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Pooling (subsampling)}{259}{section*.531}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Typical CNN architecture}{259}{section*.533}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{More on convolution}{260}{section*.535}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Bounding Box}{260}{section*.537}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Transfer learning}{261}{section*.540}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Variable length, sequences and causality}{261}{section*.542}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Exploiting time invariance}{261}{section*.543}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{LSTM and GRU}{262}{section*.545}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Different ways of processing time series}{262}{section*.547}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras basic layers}{262}{section*.549}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{More on LSTM}{263}{section*.551}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Using LSTM}{263}{section*.553}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 3}}{264}{section*.555}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 4}}{264}{section*.556}\protected@file@percent }
\@setckpt{machine_learning}{
\setcounter{page}{265}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{1}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{12}
\setcounter{table}{0}
\setcounter{pp@next@reset}{0}
\setcounter{parentequation}{0}
\setcounter{AM@survey}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{Item}{48}
\setcounter{Hfootnote}{11}
\setcounter{bookmark@seq@number}{390}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcbrastercolumn}{0}
\setcounter{tcbrasterrow}{0}
\setcounter{tcbraster}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{FancyVerbLine}{8}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{264}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{float@type}{32}
\setcounter{minted@FancyVerbLineTemp}{15}
\setcounter{minted@pygmentizecounter}{211}
\setcounter{listing}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
