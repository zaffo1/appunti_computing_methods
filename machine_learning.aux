\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Machine Learning}{205}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\textit  {Gio 3 novembre - Lezione 12}}{205}{section*.392}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Introduction to Machine Learning}{205}{section*.393}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Topics}{205}{section*.394}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Machine Learning Basics}{205}{section*.395}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Types of typical ML problems}{206}{section*.396}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Function approximation}{207}{section*.397}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Model and Hyper-parameters}{207}{section*.399}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Parameters}{207}{section*.400}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Objective function}{207}{section*.401}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Function approximation}{208}{section*.402}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Objective function: binary cross entropy}{208}{section*.403}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Learning / Training}{208}{section*.406}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised learning}{209}{section*.407}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Unsupervised learning}{209}{section*.410}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Supervised vs unsupervised}{210}{section*.412}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Reinforcement learning (not covered in this lectures)}{210}{section*.415}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Capacity and representational power}{211}{section*.417}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Generalization}{212}{section*.421}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Regularization}{212}{section*.424}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Hyperparameters (model) optimization}{213}{section*.426}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{K-folding cross validation}{213}{section*.428}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Nella figura i dati sono divisi solo in 2 (test e training), ma si possono dividere in 3 come abbiamo visto prima\relax }}{214}{figure.caption.429}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Inference}{214}{section*.430}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Accuracy, Precision, Sensitivity, Specificity}{214}{section*.431}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Examples of ML techniques}{215}{section*.433}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Linear regression (Supervised)}{215}{section*.434}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Principal Component Analysis (aka PCA) (Unsupervised)}{215}{section*.435}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nearest neighbors}{216}{section*.438}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  Figures from \url  {https://scikit-learn.org/stable/modules/neighbors.html}. \relax }}{216}{figure.caption.439}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Decision trees}{216}{section*.440}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Ensembles of trees}{217}{section*.442}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Bagging\relax }}{217}{figure.caption.444}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Gradient Boosting\relax }}{217}{figure.caption.445}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Limitations of decision trees}{218}{section*.446}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Decision Trees tools}{218}{section*.448}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Many more ML techniques!}{219}{section*.450}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{What do we need to create our first ML program}{219}{section*.452}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Hands-on}{219}{section*.453}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Lun 7 novembre - lezione 13}}{246}{section*.454}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Introduction to Artificial Neural Networks}{246}{section*.455}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Artificial Neural Networks}{246}{section*.456}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{(Artificial) neural networks: the “Model”}{246}{section*.457}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Brief history, highs and lows}{247}{section*.459}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Complexity growth}{247}{section*.461}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Performance on classic problems}{248}{section*.463}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{My favorite performance examples}{248}{section*.465}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{OpenAI GPT3}{248}{section*.466}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Neural Nets Basic elements}{248}{section*.467}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{A neural network node: the artificial neuron}{248}{section*.468}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{The MLP model}{249}{section*.470}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Universal approximation theorem}{249}{section*.472}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Example (1-D input)}{250}{section*.473}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces \url  {https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6}\relax }}{250}{figure.caption.475}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training of an MLP}{250}{section*.476}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training a NN}{251}{section*.478}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{How to find a minimum?}{251}{section*.480}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Not as simple as you would imagine}{251}{section*.482}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Learning rate, epochs and batches}{252}{section*.484}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Training and overfitting}{253}{section*.487}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Neural Networks, computers and mathematics}{253}{section*.489}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Back-propagation}{253}{section*.490}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Deep Networks}{254}{section*.492}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Deep Feed Forward networks}{254}{section*.493}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Why going deeper?}{254}{section*.495}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Activation functions}{255}{section*.496}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Deep architectures}{256}{section*.498}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Dropout and regularization methods}{257}{section*.500}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{(Batch) normalization}{257}{section*.502}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{DNN Tools}{258}{section*.504}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras}{258}{section*.505}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Other common tools}{258}{section*.506}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras Sequential example}{258}{section*.507}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras “Model” Functional API}{258}{section*.509}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{A (modernized) MLP in keras}{259}{section*.511}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{From the 1995 to 2010}{259}{section*.513}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Training a model with Keras}{260}{section*.514}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Keras Layers}{260}{section*.515}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras basic layers}{260}{section*.516}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Callbacks}{261}{section*.517}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 1}}{261}{section*.518}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 2}}{261}{section*.520}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Giovedì 10 novembre - Lezione 14}}{263}{section*.522}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Convolutional and recurrent networks}{263}{section*.523}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Classification of images}{263}{section*.524}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Exploit invariance and locality}{263}{section*.527}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Can we exploit problem invariance?}{264}{section*.529}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Limitations}{264}{section*.531}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Understanding the dimensions of the convolution}{264}{section*.532}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Pooling (subsampling)}{265}{section*.534}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Typical CNN architecture}{265}{section*.536}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{More on convolution}{265}{section*.538}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Bounding Box}{266}{section*.540}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Transfer learning}{266}{section*.543}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Variable length, sequences and causality}{267}{section*.545}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Exploiting time invariance}{267}{section*.546}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{LSTM and GRU}{267}{section*.548}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Different ways of processing time series}{268}{section*.550}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Keras basic layers}{268}{section*.552}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{More on LSTM}{269}{section*.554}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Using LSTM}{269}{section*.556}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 3}}{269}{section*.558}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 4}}{270}{section*.559}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Lunedì 14 novembre - lezione 15}}{271}{section*.560}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Autoencoders and Generative Networks}{271}{section*.561}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Dimensionality reduction task}{271}{section*.562}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Autoencoder example}{271}{section*.565}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Generative models}{272}{section*.567}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Generative Adversarial Networks}{273}{section*.569}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{GAN progress}{273}{section*.571}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces 2014: “dogs with three heads”\relax }}{273}{figure.caption.572}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces 2018: coherent generation of faces. See also \url  {https://thispersondoesnotexist.com/}\relax }}{274}{figure.caption.573}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces 2019: re-create a playable video game just by looking at videos of an existing one (so far PacMan). \hskip 1em\relax 2021: GANTheftAuto\relax }}{274}{figure.caption.574}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 5}}{274}{section*.575}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Assignment 6}}{275}{section*.577}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\textit  {Giovedì 17 novembre - Lezione 16}}{276}{section*.579}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Graph Neural Networks}{276}{section*.580}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Why graph networks?}{276}{section*.581}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Example of datasets}{276}{section*.583}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Graph structures in physics}{276}{section*.585}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Sparse datasets}{277}{section*.587}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Locality and invariance}{277}{section*.589}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Adjacency matrix}{278}{section*.591}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Data on a graph}{278}{section*.593}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Tasks}{279}{section*.595}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Learn new representation on nodes}{279}{section*.597}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Update of edges, nodes, global state}{280}{section*.599}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Propagate information}{280}{section*.601}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Message passing}{280}{section*.603}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{How/what do we train?}{281}{section*.605}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Example: b-tag at LHC}{281}{section*.607}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example: b-tag at LHC with intermediate targets}{281}{section*.609}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example: b-tag at LHC with intermediate targets}{282}{section*.611}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Tools}{282}{section*.613}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{How do we build a network in pytorch?}{284}{section*.615}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{How do we train a network in pytorch?}{284}{section*.616}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Exercise: our first GNN with pytorch}{284}{section*.617}\protected@file@percent }
\@setckpt{machine_learning}{
\setcounter{page}{286}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{4}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{0}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{21}
\setcounter{table}{0}
\setcounter{pp@next@reset}{0}
\setcounter{parentequation}{0}
\setcounter{AM@survey}{0}
\setcounter{caption@flags}{4}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{Item}{56}
\setcounter{Hfootnote}{11}
\setcounter{bookmark@seq@number}{423}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{tcbrastercolumn}{0}
\setcounter{tcbrasterrow}{0}
\setcounter{tcbraster}{0}
\setcounter{lstnumber}{1}
\setcounter{tcblisting}{0}
\setcounter{FancyVerbLine}{18}
\setcounter{linenumber}{1}
\setcounter{LN@truepage}{285}
\setcounter{FV@TrueTabGroupLevel}{0}
\setcounter{FV@TrueTabCounter}{0}
\setcounter{FV@HighlightLinesStart}{0}
\setcounter{FV@HighlightLinesStop}{0}
\setcounter{FancyVerbLineBreakLast}{0}
\setcounter{float@type}{32}
\setcounter{minted@FancyVerbLineTemp}{15}
\setcounter{minted@pygmentizecounter}{213}
\setcounter{listing}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
